from flask import Flask, render_template
from flask_sock import Sock
import json
import base64
import audioop
import wave
from datetime import datetime
import os
import sys
import numpy as np
import torch
import librosa
import soundfile as sf
import requests

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)

# Denoiser ê²½ë¡œ ì¶”ê°€
denoiser_directory = os.path.join(BASE_DIR, 'src', 'denoiser')
sys.path.append(denoiser_directory)

from denoiser import pretrained
import nemo.collections.asr as nemo_asr
from transformers import AutoModelForCausalLM, AutoTokenizer
import re
import unicodedata

# KenLM import
try:
    import kenlm
    HAS_KENLM = True
except ImportError:
    HAS_KENLM = False
    print("âš ï¸ kenlmì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Greedy ë””ì½”ë”©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# ì„¤ì • íŒŒì¼ import
try:
    from config import (
        HTTP_SERVER_PORT,
        SAMPLE_RATE_INPUT,
        SAMPLE_RATE_TARGET,
        CHUNK_DURATION,
        CHUNK_OVERLAP,
        MIN_AUDIO_LENGTH,
        MIN_ENERGY_THRESHOLD,
        AUDIO_NORMALIZATION,
        DENOISE_DRY_MIX,
        ENABLE_BEAM_SEARCH,
        BEAM_DECODER_TYPE,
        BEAM_WIDTH,
        LM_ALPHA,
        LM_BETA,
        BEAM_TOPK,
        DEBUG_BEAM_SEARCH,
        KENLM_MODEL_PATH,
        DENOISER_MODEL_PATH,
        ASR_MODEL_PATH,
        KEYWORD_MODEL_PATH,
        RECORDINGS_DIR,
        MAINBACKEND_URL,
        MAINBACKEND_ENABLED,
        MAINBACKEND_TIMEOUT
    )
except ImportError:
    # config.pyê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
    HTTP_SERVER_PORT = 5000
    SAMPLE_RATE_INPUT = 8000
    SAMPLE_RATE_TARGET = 16000
    CHUNK_DURATION = 4.0
    CHUNK_OVERLAP = 0.5
    MIN_AUDIO_LENGTH = 0.3
    MIN_ENERGY_THRESHOLD = 0.01
    AUDIO_NORMALIZATION = True
    DENOISE_DRY_MIX = 0.02
    ENABLE_BEAM_SEARCH = False
    BEAM_DECODER_TYPE = "simple"
    BEAM_WIDTH = 64
    LM_ALPHA = 0.5
    LM_BETA = 0.1
    BEAM_TOPK = 100
    DEBUG_BEAM_SEARCH = False
    KENLM_MODEL_PATH = os.path.join(BASE_DIR, 'models', 'korean_lm.bin')
    DENOISER_MODEL_PATH = os.path.join(BASE_DIR, 'models', 'denoiser.th')
    ASR_MODEL_PATH = os.path.join(BASE_DIR, 'models', 'Conformer-CTC-BPE.nemo')
    KEYWORD_MODEL_PATH = os.path.join(BASE_DIR, 'models', 'qwen3-1.7b')
    RECORDINGS_DIR = os.path.join(BASE_DIR, 'call_recordings')
    MAINBACKEND_URL = 'http://localhost:3000'
    MAINBACKEND_ENABLED = True
    MAINBACKEND_TIMEOUT = 5

class SimpleCTCBeamDecoder:
    """
    ë‹¨ì–´ ê²½ê³„ ê¸°ë°˜ CTC Beam Search Decoder with KenLM
    
    NeMo BPE í† í¬ë‚˜ì´ì €ì™€ ë‹¨ì–´ ê¸°ë°˜ KenLMì˜ í˜¸í™˜ì„±ì„ ìœ„í•´
    ë‹¨ì–´ê°€ ì™„ì„±ë  ë•Œë§Œ LM ìŠ¤ì½”ì–´ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, vocab, lm_path, beam_width=32, alpha=0.5, beta=0.1, topk=100, debug=False):
        """
        Args:
            vocab: vocabulary list (BPE tokens)
            lm_path: KenLM ëª¨ë¸ ê²½ë¡œ
            beam_width: beam í¬ê¸°
            alpha: LM weight (ë‹¨ì–´ ê¸°ë°˜ì´ë¯€ë¡œ ë‚®ì€ ê°’ ê¶Œì¥)
            beta: word insertion bonus (ë‹¨ì–´ ì™„ì„± ì‹œ ì ìš©)
            topk: ê° íƒ€ì„ìŠ¤í…ì—ì„œ ê³ ë ¤í•  ìƒìœ„ í† í° ìˆ˜
            debug: ë””ë²„ê·¸ ëª¨ë“œ (ìƒì„¸ ë¡œê¹…)
        """
        self.vocab = vocab
        # NeMo CTC outputs have shape [time, vocab_size + blank + padding]
        # For safety, we'll detect blank_id from actual logits shape during decode
        self.blank_id = None  # Will be set during first decode call
        self.beam_width = beam_width
        self.alpha = alpha
        self.beta = beta
        self.topk = topk
        self.debug = debug
        
        # KenLM ë¡œë“œ
        if HAS_KENLM and lm_path and os.path.exists(lm_path):
            self.lm = kenlm.Model(lm_path)
            try:
                log(f"  [OK] KenLM loaded: {os.path.basename(lm_path)}")
            except:
                pass
        else:
            self.lm = None
            try:
                log("  [WARN] KenLM not available, using CTC only")
            except:
                pass
    
    def _compute_lm_score(self, completed_words):
        """ì™„ì„±ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ LM ìŠ¤ì½”ì–´ ê³„ì‚°"""
        if not self.lm or not completed_words:
            return 0.0
        
        sentence = ' '.join(completed_words)
        if not sentence:
            return 0.0
        
        # KenLM ìŠ¤ì½”ì–´ + ë‹¨ì–´ ê°œìˆ˜ ë³´ë„ˆìŠ¤
        lm_prob = self.lm.score(sentence, bos=True, eos=False)
        word_bonus = self.beta * len(completed_words)
        return self.alpha * lm_prob + word_bonus
    
    def decode(self, log_probs):
        """
        CTC log probabilitiesë¥¼ ë‹¨ì–´ ê²½ê³„ ê¸°ë°˜ìœ¼ë¡œ ë””ì½”ë”©
        
        Args:
            log_probs: numpy array [time, vocab_size]
            
        Returns:
            decoded text (str)
        """
        T, V = log_probs.shape
        
        # Auto-detect blank_id on first call
        if self.blank_id is None:
            # Blank is typically the last valid token
            self.blank_id = V - 1
            try:
                log(f"[BeamSearch] Auto-detected blank_id: {self.blank_id} (vocab_size={len(self.vocab)}, logits_size={V})")
            except:
                pass
        
        if self.debug:
            try:
                log(f"[BeamSearch] Starting decode: T={T}, V={V}, blank_id={self.blank_id}")
            except:
                pass
        
        # Beam state: {key: (ctc_score, lm_score, last_token_id, current_word, completed_words)}
        # key = (tuple of completed words, current word building)
        initial_key = (tuple(), '')
        beams = {initial_key: (0.0, 0.0, None)}
        
        for t in range(T):
            probs = log_probs[t]
            
            # Top-K pruning for speed
            top_k_ids = np.argsort(probs)[-self.topk:]
            
            if self.debug and t < 3:  # ì²˜ìŒ 3 íƒ€ì„ìŠ¤í…ë§Œ ë¡œê¹…
                top_3 = np.argsort(probs)[-3:][::-1]
                top_tokens = [(i, self.vocab[i] if i < len(self.vocab) else '<unk>', probs[i]) 
                             for i in top_3]
                log(f"  [t={t}] Top-3: {top_tokens}")
            
            new_beams = {}
            
            for (completed_tuple, current_word), (ctc_score, lm_score, last_token_id) in beams.items():
                for token_id in top_k_ids:
                    token_prob = probs[token_id]
                    new_ctc = ctc_score + token_prob
                    
                    if token_id == self.blank_id:
                        # Blank: ìƒíƒœ ìœ ì§€
                        key = (completed_tuple, current_word)
                        if key not in new_beams or new_ctc + lm_score > new_beams[key][0] + new_beams[key][1]:
                            new_beams[key] = (new_ctc, lm_score, None)
                    
                    elif token_id == last_token_id:
                        # CTC collapse: ê°™ì€ í† í° ì—°ì†
                        key = (completed_tuple, current_word)
                        if key not in new_beams or new_ctc + lm_score > new_beams[key][0] + new_beams[key][1]:
                            new_beams[key] = (new_ctc, lm_score, token_id)
                    
                    else:
                        # ìƒˆ í† í° ì¶”ê°€
                        token = self.vocab[token_id] if token_id < len(self.vocab) else ''
                        
                        if not token:
                            continue
                        
                        # â–ë¡œ ì‹œì‘í•˜ë©´ ìƒˆ ë‹¨ì–´ ì‹œì‘
                        if token.startswith('â–'):
                            # í˜„ì¬ ë‹¨ì–´ë¥¼ ì™„ì„±í•˜ê³  ìƒˆ ë‹¨ì–´ ì‹œì‘
                            new_completed = list(completed_tuple)
                            if current_word:  # ì´ì „ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì™„ì„±
                                new_completed.append(current_word)
                            
                            # ìƒˆ ë‹¨ì–´ ì‹œì‘ (â– ì œê±°)
                            new_current = token.replace('â–', '')
                            
                            # LM ìŠ¤ì½”ì–´ ì¬ê³„ì‚° (ë‹¨ì–´ê°€ ì™„ì„±ë˜ì—ˆìœ¼ë¯€ë¡œ)
                            new_lm = self._compute_lm_score(new_completed) if new_completed else 0.0
                            
                            if self.debug and t < 5 and new_completed:
                                log(f"  [t={t}] Word completed: '{current_word}' â†’ {new_completed[-1]}, LM={new_lm:.2f}")
                        
                        else:
                            # subword ì¶”ê°€ (ë‹¨ì–´ ê³„ì† êµ¬ì„± ì¤‘)
                            new_completed = completed_tuple
                            new_current = current_word + token
                            new_lm = lm_score  # LM ìŠ¤ì½”ì–´ ìœ ì§€ (ë‹¨ì–´ ë¯¸ì™„ì„±)
                        
                        key = (tuple(new_completed), new_current)
                        if key not in new_beams or new_ctc + new_lm > new_beams[key][0] + new_beams[key][1]:
                            new_beams[key] = (new_ctc, new_lm, token_id)
            
            # Beam pruning: top beam_widthë§Œ ìœ ì§€
            beams = dict(sorted(new_beams.items(), 
                               key=lambda x: x[1][0] + x[1][1],  # ctc + lm
                               reverse=True)[:self.beam_width])
            
            if self.debug and t < 3:
                top_beam = sorted(beams.items(), key=lambda x: x[1][0] + x[1][1], reverse=True)[0]
                (comp, curr), (ctc, lm, _) = top_beam
                log(f"  [t={t}] Best: completed={list(comp)}, current='{curr}', ctc={ctc:.2f}, lm={lm:.2f}")
        
        # Best hypothesis ì„ íƒ ë° ìµœì¢… ì²˜ë¦¬
        if not beams:
            return ""
        
        best_key, (ctc_score, lm_score, _) = max(beams.items(), key=lambda x: x[1][0] + x[1][1])
        completed_words, current_word = best_key
        
        # ë§ˆì§€ë§‰ ë‹¨ì–´ ì¶”ê°€ (ì•„ì§ ì™„ì„± ì•ˆëœ ë‹¨ì–´)
        final_words = list(completed_words)
        if current_word:
            final_words.append(current_word)
        
        result = ' '.join(final_words)
        
        if self.debug:
            log(f"[BeamSearch] Final result: '{result}' (ctc={ctc_score:.2f}, lm={lm_score:.2f})")
        
        return result

app = Flask(__name__)
sock = Sock(app)

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì €ì¥
denoiser_model = None
asr_model = None
keyword_model = None
keyword_tokenizer = None
device = None
ctc_decoder = None  # SimpleCTC Beam Search ë””ì½”ë”
USE_BEAM_SEARCH = False  # Beam Search ì‚¬ìš© ì—¬ë¶€
BEAM_DECODER_MODE = "simple"  # "simple" or "nemo"

def log(msg, *args):
    print(f"Media WS: ", msg, *args)

def notify_call_start(call_info):
    """MainBackendì— í†µí™” ì‹œì‘ ì•Œë¦¼"""
    if not MAINBACKEND_ENABLED:
        return
    
    try:
        response = requests.post(
            f'{MAINBACKEND_URL}/api/stt/call-start',
            json={
                'callId': call_info['call_sid'],
                'phoneNumber': call_info['from_number'],
                'timestamp': call_info['timestamp']
            },
            timeout=MAINBACKEND_TIMEOUT
        )
        if response.status_code == 200:
            log(f"âœ“ Call start notified to MainBackend")
        else:
            log(f"âœ— MainBackend error: {response.status_code}")
    except Exception as e:
        log(f"âœ— Failed to notify MainBackend: {e}")

def send_transcription_to_mainbackend(call_sid, speaker, text, keywords):
    """MainBackendì— ì‹¤ì‹œê°„ ì „ì‚¬ ê²°ê³¼ ì „ì†¡"""
    if not MAINBACKEND_ENABLED or not text:
        return
    
    speaker_map = {'inbound': 'customer', 'outbound': 'agent'}
    
    try:
        response = requests.post(
            f'{MAINBACKEND_URL}/api/stt/line',
            json={
                'callId': call_sid,
                'speaker': speaker_map.get(speaker, 'customer'),
                'text': text,
                'keywords': keywords or []
            },
            timeout=MAINBACKEND_TIMEOUT
        )
        if response.status_code == 200:
            log(f"âœ“ Sent to MainBackend [{speaker}]: {text[:50]}")
        else:
            log(f"âœ— MainBackend error: {response.status_code}")
    except requests.exceptions.Timeout:
        log(f"âœ— MainBackend timeout (>{MAINBACKEND_TIMEOUT}s)")
    except Exception as e:
        log(f"âœ— Failed to send to MainBackend: {e}")

def notify_call_end(call_sid):
    """MainBackendì— í†µí™” ì¢…ë£Œ ì•Œë¦¼"""
    if not MAINBACKEND_ENABLED:
        return
    
    try:
        response = requests.post(
            f'{MAINBACKEND_URL}/call/end',
            json={'callId': call_sid},
            timeout=MAINBACKEND_TIMEOUT
        )
        if response.status_code == 200:
            log(f"âœ“ Call end notified to MainBackend")
        else:
            log(f"âœ— MainBackend error: {response.status_code}")
    except Exception as e:
        log(f"âœ— Failed to notify call end: {e}")

def load_models():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global denoiser_model, asr_model, keyword_model, keyword_tokenizer, device, ctc_decoder, USE_BEAM_SEARCH, BEAM_DECODER_MODE
    
    log("Loading models...")
    
    # Device ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    log(f"Using device: {device}")
    
    # Denoiser ëª¨ë¸ ë¡œë“œ
    try:
        import argparse
        denoiser_args = argparse.Namespace(
            dns64=False,
            dns48=False,
            master64=False,
            device=str(device),
            dry=DENOISE_DRY_MIX,  # ë” ê°•í•œ ë…¸ì´ì¦ˆ ì œê±° (0.04 â†’ 0.02)
            model_path=DENOISER_MODEL_PATH
        )
        denoiser_model = pretrained.get_model(denoiser_args).to(device)
        denoiser_model.eval()
        log("âœ“ Denoiser model loaded successfully")
    except Exception as e:
        log(f"Warning: Could not load denoiser model: {e}")
        denoiser_model = None
    
    # ASR ëª¨ë¸ ë¡œë“œ
    try:
        asr_model = nemo_asr.models.EncDecCTCModelBPE.restore_from(ASR_MODEL_PATH, map_location=device)
        asr_model.eval()
        
        # Preprocessor ì„¤ì •
        from omegaconf import OmegaConf
        import copy
        asr_cfg = copy.deepcopy(asr_model._cfg)
        OmegaConf.set_struct(asr_cfg.preprocessor, False)
        asr_cfg.preprocessor.dither = 0.0
        asr_cfg.preprocessor.pad_to = 0
        OmegaConf.set_struct(asr_cfg.preprocessor, True)
        asr_model.preprocessor = asr_model.from_config_dict(asr_cfg.preprocessor)
        
        if device.type == 'cuda':
            asr_model.cuda()
        
        log("âœ“ ASR model loaded successfully")
        
        # Beam Search ì„¤ì •
        if ENABLE_BEAM_SEARCH:
            # KenLM ëª¨ë¸ ê²½ë¡œ í™•ì¸
            kenlm_paths = [
                KENLM_MODEL_PATH,  # config.pyì—ì„œ ì§€ì •í•œ ê²½ë¡œ
                os.path.join(BASE_DIR, 'models', 'korean_4gram.binary'),
                os.path.join(BASE_DIR, 'models', 'korean_4gram.arpa'),
                os.path.join(BASE_DIR, 'models', 'korean_lm.bin'),
            ]
            
            kenlm_model_path = None
            for path in kenlm_paths:
                if os.path.exists(path):
                    kenlm_model_path = path
                    break
            
            if kenlm_model_path and HAS_KENLM:
                BEAM_DECODER_MODE = BEAM_DECODER_TYPE
                
                if BEAM_DECODER_TYPE == "nemo":
                    # NeMo ê³µì‹ BeamCTCInfer ì‚¬ìš©
                    try:
                        from nemo.collections.asr.parts.submodules import ctc_beam_decoding
                        
                        # BeamCTCInferConfig ì„¤ì •
                        beam_config = ctc_beam_decoding.BeamCTCInferConfig(
                            beam_size=BEAM_WIDTH,
                            beam_alpha=LM_ALPHA,
                            beam_beta=LM_BETA,
                            kenlm_path=kenlm_model_path,
                            return_best_hypothesis=True
                        )
                        
                        # ASR ëª¨ë¸ì— decoding strategy ì„¤ì •
                        asr_model.cfg.decoding.strategy = "beam"
                        asr_model.cfg.decoding.beam = beam_config
                        asr_model.change_decoding_strategy(asr_model.cfg.decoding)
                        
                        USE_BEAM_SEARCH = True
                        log("âœ… NeMo BeamCTCDecoder initialized successfully")
                        log(f"   - Official NeMo implementation")
                        log(f"   - Optimized for CTC + KenLM")
                        log(f"   - KenLM model: {os.path.basename(kenlm_model_path)}")
                        log(f"   - Beam width: {BEAM_WIDTH}")
                        log(f"   - Alpha (LM weight): {LM_ALPHA}")
                        log(f"   - Beta (word bonus): {LM_BETA}")
                    except Exception as e:
                        log(f"Warning: NeMo decoder failed, falling back to SimpleCTC: {e}")
                        BEAM_DECODER_MODE = "simple"
                
                if BEAM_DECODER_MODE == "simple":
                    # SimpleCTCBeamDecoder ì‚¬ìš© (fallback)
                    try:
                        # Vocabulary ë¡œë“œ
                        vocab_path = os.path.join(BASE_DIR, 'src', 'nemo_asr', 'tokenizer_spe_bpe_v2048', 'vocab.txt')
                        with open(vocab_path, 'r', encoding='utf-8') as f:
                            vocab_list = [line.strip() for line in f]
                        log(f"âœ“ Loaded vocabulary: {len(vocab_list)} tokens")
                        
                        ctc_decoder = SimpleCTCBeamDecoder(
                            vocab=vocab_list,
                            lm_path=kenlm_model_path,
                            beam_width=BEAM_WIDTH,
                            alpha=LM_ALPHA,
                            beta=LM_BETA,
                            topk=BEAM_TOPK,
                            debug=DEBUG_BEAM_SEARCH
                        )
                        USE_BEAM_SEARCH = True
                        log("âœ… SimpleCTCBeamDecoder initialized successfully")
                        log(f"   - Word-boundary based LM integration")
                        log(f"   - Pure Python implementation (Windows compatible)")
                        log(f"   - KenLM model: {os.path.basename(kenlm_model_path)}")
                        log(f"   - Beam width: {BEAM_WIDTH}")
                        log(f"   - Alpha (LM weight): {LM_ALPHA}")
                        log(f"   - Beta (word bonus): {LM_BETA}")
                        log(f"   - Top-K pruning: {BEAM_TOPK}")
                        if DEBUG_BEAM_SEARCH:
                            log(f"   - Debug mode: ENABLED")
                    except Exception as e:
                        log(f"Warning: Could not initialize SimpleCTC decoder: {e}")
                        import traceback
                        traceback.print_exc()
                        USE_BEAM_SEARCH = False
            else:
                if not kenlm_model_path:
                    log("[INFO] KenLM model not found at any of these paths:")
                    for path in kenlm_paths:
                        log(f"      - {path}")
                    log("      Using Greedy decoding")
                elif not HAS_KENLM:
                    log("[INFO] kenlm not available, using Greedy decoding")
                    log("      Install: pip install https://github.com/kpu/kenlm/archive/master.zip")
                USE_BEAM_SEARCH = False
        else:
            log("[INFO] Beam Search disabled in config (ENABLE_BEAM_SEARCH=False)")
            log("      Using Greedy decoding for stability")
            USE_BEAM_SEARCH = False
            
    except Exception as e:
        log(f"Warning: Could not load ASR model: {e}")
        asr_model = None
        ctc_decoder = None
        USE_BEAM_SEARCH = False
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ ë¡œë“œ (Qwen3-1.7B)
    try:
        # ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        model_files = ['pytorch_model.bin', 'model.safetensors']
        has_model_weights = False
        
        if os.path.exists(KEYWORD_MODEL_PATH):
            # ì‹¤ì œ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            has_model_weights = any(
                os.path.exists(os.path.join(KEYWORD_MODEL_PATH, f)) 
                for f in model_files
            )
        
        if has_model_weights:
            # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
            keyword_model_path = KEYWORD_MODEL_PATH
            log(f"Loading keyword extraction model from local: {KEYWORD_MODEL_PATH}")
        else:
            # HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ (Qwen3-1.7B ì‚¬ìš©)
            keyword_model_path = "Qwen/Qwen3-1.7B"
            log(f"Local model weights not found. Downloading from HuggingFace: {keyword_model_path}")
            log(f"  Note: First download will take 5-10 minutes (~1.7GB)")
        
        keyword_tokenizer = AutoTokenizer.from_pretrained(keyword_model_path)
        keyword_model = AutoModelForCausalLM.from_pretrained(
            keyword_model_path,
            torch_dtype="auto",
            device_map="auto"
        )
        log("âœ“ Keyword extraction model loaded successfully")
        log(f"  - Model source: {'Local' if has_model_weights else 'HuggingFace'}")
        log(f"  - Model path: {keyword_model_path}")
    except Exception as e:
        log(f"Warning: Could not load keyword model: {e}")
        keyword_model = None
        keyword_tokenizer = None
    
    log("All models loaded and ready!")

@app.route("/", methods=["GET"])
def index():
    return "OK", 200

@app.route('/twiml', methods=['GET', 'POST'])
def return_twiml():
    from flask import request
    print("POST TwiML")
    
    # Twilioì—ì„œ ì „ë‹¬í•˜ëŠ” ìš”ì²­ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    from_number = request.values.get('From', 'Unknown')
    to_number = request.values.get('To', 'Unknown')
    call_sid = request.values.get('CallSid', 'Unknown')
    
    log(f"TwiML ìš”ì²­ ë°›ìŒ - From: {from_number}, To: {to_number}, CallSid: {call_sid}")
    
    # í…œí”Œë¦¿ì— íŒŒë¼ë¯¸í„° ì „ë‹¬
    return render_template('streams.xml', From=from_number, To=to_number, CallSid=call_sid)

@sock.route("/stream")
def echo(ws):
    log("Connection accepted")
    count = 0
    has_seen_media = False
    
    # í†µí™” ì •ë³´ ì €ì¥ ê°ì²´
    call_info = {
        'from_number': None,
        'to_number': None,
        'call_sid': None,
        'stream_sid': None,
        'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S")
    }
    
    # í™”ìë³„ ì´ì¤‘ ë²„í¼ êµ¬ì¡°
    buffers = {
        'inbound': {  # ê³ ê°
            'audio': [],           # ì €ì¥ìš© ë²„í¼
            'processing': [],      # ì‹¤ì‹œê°„ ì²˜ë¦¬ìš© ë²„í¼
            'transcriptions': [],  # ì „ì‚¬ ê²°ê³¼
            'keywords': []         # ì¶”ì¶œëœ í‚¤ì›Œë“œ
        },
        'outbound': {  # ìƒë‹´ì‚¬
            'audio': [],
            'processing': [],
            'transcriptions': [],
            'keywords': []
        }
    }
    
    # ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
    CHUNK_SIZE = int(SAMPLE_RATE_INPUT * CHUNK_DURATION)  # ìƒ˜í”Œ ìˆ˜
    OVERLAP_SIZE = int(CHUNK_SIZE * CHUNK_OVERLAP)  # ì˜¤ë²„ë© ìƒ˜í”Œ ìˆ˜
    
    # í™”ì ë¼ë²¨ ë§¤í•‘
    speaker_labels = {
        'inbound': 'ê³ ê°',
        'outbound': 'ìƒë‹´ì‚¬'
    }
    
    while True:
        try:
            message = ws.receive()
            if message is None:
                log("No message received...")
                break
            
            data = json.loads(message)
            
            if data['event'] == "connected":
                log("Connected Message received")
                
            if data['event'] == "start":
                log("Start Message received")
                
                # ë””ë²„ê¹…: start ì´ë²¤íŠ¸ì˜ ì „ì²´ ë°ì´í„° êµ¬ì¡° ì¶œë ¥
                log("DEBUG - Full start event data:")
                log(json.dumps(data, indent=2, ensure_ascii=False))
                
                # í†µí™” ì •ë³´ ì¶”ì¶œ
                start_data = data.get('start', {})
                call_info['stream_sid'] = start_data.get('streamSid')
                call_info['call_sid'] = start_data.get('callSid')
                
                # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (customParameters ë˜ëŠ” ì§ì ‘ í•„ë“œì—ì„œ)
                custom_params = start_data.get('customParameters', {})
                call_info['from_number'] = custom_params.get('From') or start_data.get('from')
                call_info['to_number'] = custom_params.get('To') or start_data.get('to')
                
                # ì „í™”ë²ˆí˜¸ ì¶œë ¥ (1íšŒë§Œ)
                log("=" * 60)
                log("ğŸ“ í†µí™” ì •ë³´")
                log("=" * 60)
                if call_info['from_number']:
                    log(f"ë°œì‹  ë²ˆí˜¸ (From): {call_info['from_number']}")
                if call_info['to_number']:
                    log(f"ìˆ˜ì‹  ë²ˆí˜¸ (To): {call_info['to_number']}")
                if call_info['call_sid']:
                    log(f"í†µí™” ID (Call SID): {call_info['call_sid']}")
                if call_info['stream_sid']:
                    log(f"ìŠ¤íŠ¸ë¦¼ ID (Stream SID): {call_info['stream_sid']}")
                log("=" * 60)
                
                # MainBackend í†µí™” ì‹œì‘ ì•Œë¦¼
                notify_call_start(call_info)
                
                log("Starting real-time dual-track Denoise + STT processing...")
                log("Track: inbound (ê³ ê°) / outbound (ìƒë‹´ì‚¬)")
                
            if data['event'] == "media":
                if not has_seen_media:
                    log("Media messages received - processing started")
                    has_seen_media = True
                
                # track í•„ë“œë¡œ í™”ì êµ¬ë¶„
                track = data['media'].get('track', 'inbound_track')
                
                # ë””ë²„ê¹…: track ê°’ í™•ì¸ (ì²˜ìŒ ëª‡ ê°œë§Œ ì¶œë ¥)
                if count < 5:
                    log(f"DEBUG: Received track value: '{track}'")
                
                # track ê°’ì— ë”°ë¼ í™”ì êµ¬ë¶„
                if 'inbound' in track.lower():
                    speaker = 'inbound'
                elif 'outbound' in track.lower():
                    speaker = 'outbound'
                else:
                    # ê¸°ë³¸ê°’ì€ inboundë¡œ ì„¤ì •
                    speaker = 'inbound'
                    if count < 5:
                        log(f"WARNING: Unknown track value '{track}', defaulting to inbound")
                
                # base64 ë””ì½”ë”©
                payload = data['media']['payload']
                audio_data = base64.b64decode(payload)
                
                # mu-lawë¥¼ PCMìœ¼ë¡œ ë³€í™˜ (8bit mu-law -> 16bit PCM)
                pcm_data = audioop.ulaw2lin(audio_data, 2)
                
                # í•´ë‹¹ í™”ìì˜ ë²„í¼ì— ì¶”ê°€
                buffers[speaker]['audio'].append(pcm_data)
                buffers[speaker]['processing'].append(pcm_data)
                
                # ë²„í¼ê°€ ì¶©ë¶„íˆ ìŒ“ì´ë©´ ì²˜ë¦¬
                current_size = sum(len(chunk) for chunk in buffers[speaker]['processing'])
                if current_size >= CHUNK_SIZE * 2:  # 16-bit = 2 bytes per sample
                    # ì‹¤ì‹œê°„ ì²˜ë¦¬
                    try:
                        transcription = process_audio_chunk(
                            buffers[speaker]['processing'], 
                            SAMPLE_RATE_INPUT, 
                            SAMPLE_RATE_TARGET
                        )
                        
                        # ë¹ˆ ë¬¸ìì—´, ë„ˆë¬´ ì§§ì€ ê²°ê³¼, ë°˜ë³µë˜ëŠ” ë‹¨ì¼ ìŒì ˆ í•„í„°ë§
                        if transcription and len(transcription.strip()) > 1:
                            # ë‹¨ì¼ ìŒì ˆ í•„í„°ë§ (ì˜ˆ: "ì˜¤", "ìŒ", "ì•„")
                            single_syllables = ['ì˜¤', 'ìŒ', 'ì•„', 'ì–´', 'ìœ¼', 'ì´', 'ì—', 'ì™€', 'í•˜']
                            is_single_syllable = any(
                                transcription.strip() == syllable for syllable in single_syllables
                            )
                            
                            # ì¤‘ë³µ ì²´í¬ ì¶”ê°€ (ì˜¤ë²„ë©ìœ¼ë¡œ ì¸í•œ ë°˜ë³µ ì œê±°)
                            is_duplicate = is_duplicate_transcription(
                                transcription,
                                buffers[speaker]['transcriptions'],
                                similarity_threshold=0.7  # 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                            )
                            
                            if not is_single_syllable and not is_duplicate:
                                buffers[speaker]['transcriptions'].append(transcription)
                                log(f"[{speaker_labels[speaker]}] Transcription: {transcription}")
                                
                                # í‚¤ì›Œë“œ ì¶”ì¶œ
                                keywords = extract_keywords(transcription)
                                if keywords:
                                    buffers[speaker]['keywords'].extend(keywords)
                                    log(f"[{speaker_labels[speaker]}] ğŸ”‘ Keywords: {keywords}")
                                
                                # MainBackend ì „ì†¡
                                send_transcription_to_mainbackend(
                                    call_info.get('call_sid'),
                                    speaker,
                                    transcription,
                                    keywords
                                )
                    except Exception as e:
                        log(f"[{speaker_labels[speaker]}] Error processing chunk: {e}")
                    
                    # ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ CHUNK_OVERLAP ë¹„ìœ¨ë§Œí¼ ìœ ì§€
                    total_bytes = sum(len(chunk) for chunk in buffers[speaker]['processing'])
                    keep_bytes = int(total_bytes * CHUNK_OVERLAP)
                    
                    if keep_bytes > 0:
                        # ë’¤ì—ì„œë¶€í„° keep_bytesë§Œí¼ ìœ ì§€
                        temp_buffer = []
                        accumulated = 0
                        for chunk in reversed(buffers[speaker]['processing']):
                            if accumulated >= keep_bytes:
                                break
                            temp_buffer.insert(0, chunk)
                            accumulated += len(chunk)
                        buffers[speaker]['processing'] = temp_buffer
                    else:
                        buffers[speaker]['processing'] = []
                
            if data['event'] == "closed":
                log("Closed Message received")
                # MainBackend í†µí™” ì¢…ë£Œ ì•Œë¦¼
                notify_call_end(call_info.get('call_sid'))
                break
                
            count += 1
            
        except Exception as e:
            log(f"Error: {e}")
            import traceback
            traceback.print_exc()
            break

    log(f"Connection closed. Received a total of {count} messages")
    
    # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬ (ì–‘ìª½ í™”ì ëª¨ë‘)
    for speaker in ['inbound', 'outbound']:
        if buffers[speaker]['processing']:
            try:
                transcription = process_audio_chunk(
                    buffers[speaker]['processing'], 
                    SAMPLE_RATE_INPUT, 
                    SAMPLE_RATE_TARGET
                )
                if transcription:
                    # ë§ˆì§€ë§‰ ì²­í¬ë„ ì¤‘ë³µ ì²´í¬ ì ìš©
                    is_duplicate = is_duplicate_transcription(
                        transcription,
                        buffers[speaker]['transcriptions'],
                        similarity_threshold=0.7
                    )
                    
                    if not is_duplicate:
                        buffers[speaker]['transcriptions'].append(transcription)
                        log(f"[{speaker_labels[speaker]}] Final transcription: {transcription}")
                        
                        # ë§ˆì§€ë§‰ í‚¤ì›Œë“œ ì¶”ì¶œ
                        keywords = extract_keywords(transcription)
                        if keywords:
                            buffers[speaker]['keywords'].extend(keywords)
                            log(f"[{speaker_labels[speaker]}] ğŸ”‘ Keywords: {keywords}")
            except Exception as e:
                log(f"[{speaker_labels[speaker]}] Error processing final chunk: {e}")
    
    # í™”ìë³„ íŒŒì¼ ì €ì¥
    save_dual_track_results(buffers, speaker_labels, call_info)

def extract_keywords(text):
    """
    Qwen3-1.7Bë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ ë¬¸ì¥ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    Args:
        text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í•œêµ­ì–´ ë¬¸ì¥
        
    Returns:
        list: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    if not text or not text.strip() or keyword_model is None or keyword_tokenizer is None:
        return []
    
    try:
        system_prompt = (
            "ë‹¹ì‹ ì€ í•œêµ­ì–´ í•œ ë¬¸ì¥ì—ì„œ ê²€ìƒ‰/ë¶„ë¥˜ì— ìœ ì˜ë¯¸í•œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n"
            "ê·œì¹™:\n"
            "- í‚¤ì›Œë“œëŠ” ê³ ìœ ëª…ì‚¬, ê¸°ìˆ ëª…, ê°œë…, ê°ì²´ ì¤‘ì‹¬\n"
            "- ê°ì •, ì¶”ì„ìƒˆ, ì¼ë°˜ì ì¸ ë§ì€ ì œì™¸\n"
            "- í‚¤ì›Œë“œê°€ í•„ìš” ì—†ìœ¼ë©´ ë°˜ë“œì‹œ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜\n"
            "- ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í•œ ì¤„ë¡œë§Œ: {\"keywords\": [..]}\n"
            "- ì¶”ë¡  ê³¼ì •, ì„¤ëª…, ì¶”ê°€ ë¬¸ì¥ ê¸ˆì§€\n"
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ë¬¸ì¥: {text}"}
        ]

        text_input = keyword_tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )

        model_inputs = keyword_tokenizer([text_input], return_tensors="pt").to(keyword_model.device)

        generated_ids = keyword_model.generate(
            **model_inputs,
            max_new_tokens=128,
            min_new_tokens=5,
            do_sample=True,
            temperature=0.7,
            top_p=0.8,
            top_k=20,
            pad_token_id=keyword_tokenizer.eos_token_id
        )

        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]
        decoded_text = keyword_tokenizer.decode(output_ids, skip_special_tokens=True).strip()

        # think íƒœê·¸ ì œê±°
        decoded_text = re.sub(r'<think>.*?</think>', '', decoded_text, flags=re.DOTALL).strip()

        # JSON ì¶”ì¶œ
        m = re.search(r'\{.*\}', decoded_text, flags=re.DOTALL)
        if not m:
            return []

        result = json.loads(m.group(0))
        return result.get('keywords', [])
        
    except Exception as e:
        log(f"Error in extract_keywords: {e}")
        return []

def is_duplicate_transcription(new_text, recent_texts, similarity_threshold=0.7):
    """
    ì´ì „ ì „ì‚¬ ê²°ê³¼ì™€ ìœ ì‚¬ë„ë¥¼ ì²´í¬í•˜ì—¬ ì¤‘ë³µ ì—¬ë¶€ íŒë‹¨
    
    Args:
        new_text: ìƒˆë¡œìš´ ì „ì‚¬ ê²°ê³¼
        recent_texts: ìµœê·¼ ì „ì‚¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0~1.0)
        
    Returns:
        bool: ì¤‘ë³µì´ë©´ True, ì•„ë‹ˆë©´ False
    """
    from difflib import SequenceMatcher
    
    if not new_text or not recent_texts:
        return False
    
    # ìµœê·¼ 3ê°œì˜ ì „ì‚¬ ê²°ê³¼ì™€ë§Œ ë¹„êµ (íš¨ìœ¨ì„±)
    for prev_text in recent_texts[-3:]:
        if not prev_text:
            continue
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarity = SequenceMatcher(None, new_text.strip(), prev_text.strip()).ratio()
        
        # ì„ê³„ê°’ ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
        if similarity > similarity_threshold:
            return True
    
    return False

def process_audio_chunk(buffer, input_sr, target_sr):
    """ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ Denoise + STT ì²˜ë¦¬"""
    try:
        # ë²„í¼ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        audio_data = b''.join(buffer)
        audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´ ì²´í¬ (ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ)
        duration = len(audio_np) / input_sr
        if duration < MIN_AUDIO_LENGTH:
            return None
        
        # ìŒì„± ì—ë„ˆì§€ ì²´í¬ (ë„ˆë¬´ ì¡°ìš©í•˜ë©´ ìŠ¤í‚µ)
        rms_energy = np.sqrt(np.mean(audio_np**2))
        if rms_energy < MIN_ENERGY_THRESHOLD:
            return None
        
        # ì˜¤ë””ì˜¤ ì •ê·œí™” (ë³¼ë¥¨ ê· ì¼í™”)
        if AUDIO_NORMALIZATION:
            max_amp = np.max(np.abs(audio_np))
            if max_amp > 0:
                audio_np = audio_np / max_amp
        
        # ë¦¬ìƒ˜í”Œë§ (8kHz -> 16kHz)
        if input_sr != target_sr:
            audio_resampled = librosa.resample(audio_np, orig_sr=input_sr, target_sr=target_sr)
        else:
            audio_resampled = audio_np
        
        # Denoise
        if denoiser_model is not None:
            audio_tensor = torch.tensor(audio_resampled).unsqueeze(0).unsqueeze(0).to(device)
            with torch.no_grad():
                audio_denoised = denoiser_model(audio_tensor)
            audio_denoised = audio_denoised.squeeze().cpu().numpy()
        else:
            audio_denoised = audio_resampled
        
        # STT
        if asr_model is not None:
            with torch.no_grad():
                if USE_BEAM_SEARCH:
                    if BEAM_DECODER_MODE == "nemo":
                        # NeMo ê³µì‹ BeamCTCDecoder ì‚¬ìš©
                        try:
                            # transcribe with beam search
                            transcription = asr_model.transcribe([audio_denoised], batch_size=1)
                            if transcription and len(transcription) > 0:
                                result = transcription[0]
                                if hasattr(result, 'text'):
                                    text = result.text
                                else:
                                    text = str(result)
                                
                                if text:
                                    text = unicodedata.normalize('NFC', text)
                                    # í›„ì²˜ë¦¬: ë°˜ë³µ ë¬¸ì ì œê±°
                                    text = re.sub(r'(.)\1{2,}', r'\1\1', text)
                                    return text.strip()
                        except Exception as e:
                            log(f"NeMo Beam Search failed, falling back to Greedy: {e}")
                    
                    elif BEAM_DECODER_MODE == "simple" and ctc_decoder is not None:
                        # SimpleCTCBeamDecoder ì‚¬ìš©
                        try:
                            # audioë¥¼ tensorë¡œ ë³€í™˜
                            audio_tensor = torch.tensor(audio_denoised).unsqueeze(0).to(device)
                            audio_length = torch.tensor([audio_tensor.shape[1]]).to(device)
                            
                            # NeMo ëª¨ë¸ì—ì„œ logits ì¶”ì¶œ
                            processed_signal, processed_signal_length = asr_model.preprocessor(
                                input_signal=audio_tensor, length=audio_length
                            )
                            if asr_model.spec_augmentation is not None and asr_model.training:
                                processed_signal = asr_model.spec_augmentation(
                                    input_spec=processed_signal, length=processed_signal_length
                                )
                            encoded, encoded_len = asr_model.encoder(
                                audio_signal=processed_signal, length=processed_signal_length
                            )
                            log_probs = asr_model.decoder(encoder_output=encoded)
                            
                            # SimpleCTCBeamDecoderë¡œ ë””ì½”ë”©
                            # log_probs shape: [batch=1, time, vocab]
                            logits_np = log_probs[0].cpu().numpy()  # [time, vocab]
                            text = ctc_decoder.decode(logits_np)
                            
                            if text:
                                text = unicodedata.normalize('NFC', text)
                                # í›„ì²˜ë¦¬: ë°˜ë³µ ë¬¸ì ì œê±°
                                text = re.sub(r'(.)\1{2,}', r'\1\1', text)
                                return text.strip()
                        except Exception as e:
                            log(f"SimpleCTC Beam Search failed, falling back to Greedy: {e}")
                
                # Greedy ë””ì½”ë”© (ê¸°ë³¸ ë˜ëŠ” í´ë°±)
                # ASR ëª¨ë¸ì˜ decoding strategyë¥¼ ì„ì‹œë¡œ greedyë¡œ ë³€ê²½
                original_strategy = None
                try:
                    if hasattr(asr_model, 'cfg') and hasattr(asr_model.cfg, 'decoding'):
                        original_strategy = asr_model.cfg.decoding.strategy
                        asr_model.change_decoding_strategy(None)  # Reset to greedy
                except:
                    pass
                
                transcription = asr_model.transcribe([audio_denoised], batch_size=1)
                
                # ì›ë˜ strategy ë³µêµ¬
                if original_strategy and USE_BEAM_SEARCH and BEAM_DECODER_MODE == "nemo":
                    try:
                        asr_model.change_decoding_strategy(asr_model.cfg.decoding)
                    except:
                        pass
                
                if transcription and len(transcription) > 0:
                    # Hypothesis ê°ì²´ì—ì„œ text ì†ì„± ì¶”ì¶œ
                    result = transcription[0]
                    if hasattr(result, 'text'):
                        text = result.text
                    else:
                        text = str(result)
                    
                    if text:
                        text = unicodedata.normalize('NFC', text)
                        # í›„ì²˜ë¦¬: ë°˜ë³µ ë¬¸ì ì œê±° (ì˜ˆ: "ì˜¤ì˜¤ì˜¤" â†’ "ì˜¤ì˜¤")
                        text = re.sub(r'(.)\1{2,}', r'\1\1', text)
                        return text.strip()
        
        return None
        
    except Exception as e:
        log(f"Error in process_audio_chunk: {e}")
        import traceback
        traceback.print_exc()
        return None

def save_dual_track_results(buffers, speaker_labels, call_info):
    """í™”ìë³„ ì˜¤ë””ì˜¤, ì „ì‚¬ ê²°ê³¼, í‚¤ì›Œë“œ ì €ì¥"""
    try:
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ íŒŒì¼ëª… (call_infoì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        timestamp = call_info.get('timestamp', datetime.now().strftime("%Y%m%d_%H%M%S"))
        os.makedirs(RECORDINGS_DIR, exist_ok=True)
        
        # íŒŒì¼ëª… ë§¤í•‘
        file_suffixes = {
            'inbound': 'customer',   # ê³ ê°
            'outbound': 'agent'      # ìƒë‹´ì‚¬
        }
        
        total_duration = 0
        stats = {}
        
        # ê° í™”ìë³„ë¡œ íŒŒì¼ ì €ì¥
        for speaker in ['inbound', 'outbound']:
            suffix = file_suffixes[speaker]
            label = speaker_labels[speaker]
            
            # ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì €ì¥
            if buffers[speaker]['audio']:
                # WAV íŒŒì¼ ì €ì¥
                audio_filename = os.path.join(RECORDINGS_DIR, f"call_{timestamp}_{suffix}.wav")
                audio_data = b''.join(buffers[speaker]['audio'])
                
                with wave.open(audio_filename, 'wb') as wav_file:
                    wav_file.setnchannels(1)
                    wav_file.setsampwidth(2)
                    wav_file.setframerate(SAMPLE_RATE_INPUT)
                    wav_file.writeframes(audio_data)
                
                duration = len(audio_data) / (SAMPLE_RATE_INPUT * 2)
                total_duration = max(total_duration, duration)
                log(f"[{label}] Audio saved: {audio_filename}")
                log(f"[{label}] Duration: {duration:.2f} seconds")
                
                # í†µê³„ ì €ì¥
                stats[speaker] = {
                    'audio_file': audio_filename,
                    'duration': duration,
                    'chunks': len(buffers[speaker]['audio'])
                }
            
            # ì „ì‚¬ ê²°ê³¼ ë° í‚¤ì›Œë“œ ì €ì¥
            if buffers[speaker]['transcriptions']:
                txt_filename = os.path.join(RECORDINGS_DIR, f"call_{timestamp}_{suffix}.txt")
                with open(txt_filename, 'w', encoding='utf-8') as f:
                    # í†µí™” ì •ë³´ í—¤ë”
                    f.write("=" * 60 + "\n")
                    f.write("í†µí™” ì •ë³´\n")
                    f.write("=" * 60 + "\n")
                    if call_info.get('from_number'):
                        f.write(f"ë°œì‹  ë²ˆí˜¸: {call_info['from_number']}\n")
                    if call_info.get('to_number'):
                        f.write(f"ìˆ˜ì‹  ë²ˆí˜¸: {call_info['to_number']}\n")
                    if call_info.get('call_sid'):
                        f.write(f"í†µí™” ID: {call_info['call_sid']}\n")
                    if call_info.get('stream_sid'):
                        f.write(f"ìŠ¤íŠ¸ë¦¼ ID: {call_info['stream_sid']}\n")
                    f.write(f"ì €ì¥ ì‹œê°: {timestamp}\n")
                    f.write("=" * 60 + "\n\n")
                    
                    f.write(f"=== í™”ì: {label} ({speaker.capitalize()} Track) ===\n\n")
                    
                    f.write("=== Real-time Transcription Results ===\n\n")
                    for i, trans in enumerate(buffers[speaker]['transcriptions'], 1):
                        f.write(f"[Chunk {i}] {trans}\n")
                    
                    f.write("\n=== Full Transcription ===\n")
                    full_text = " ".join(buffers[speaker]['transcriptions'])
                    f.write(full_text)
                    
                    # í‚¤ì›Œë“œ ì¶”ê°€
                    if buffers[speaker]['keywords']:
                        f.write("\n\n=== Extracted Keywords ===\n")
                        unique_keywords = list(set(buffers[speaker]['keywords']))
                        f.write(f"Total unique keywords: {len(unique_keywords)}\n")
                        f.write(f"Keywords: {', '.join(unique_keywords)}\n")
                
                log(f"[{label}] Transcription saved: {txt_filename}")
                log(f"[{label}] Total chunks transcribed: {len(buffers[speaker]['transcriptions'])}")
                
                if buffers[speaker]['keywords']:
                    unique_keywords = list(set(buffers[speaker]['keywords']))
                    log(f"[{label}] Extracted {len(unique_keywords)} unique keywords: {unique_keywords}")
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                if speaker in stats:
                    stats[speaker]['txt_file'] = txt_filename
                    stats[speaker]['transcriptions'] = len(buffers[speaker]['transcriptions'])
                    stats[speaker]['keywords'] = len(unique_keywords) if buffers[speaker]['keywords'] else 0
        
        # ì „ì²´ í†µí™” ìš”ì•½
        log("\n=== Call Summary ===")
        log(f"Total call duration: {total_duration:.2f} seconds")
        for speaker in ['inbound', 'outbound']:
            if speaker in stats:
                label = speaker_labels[speaker]
                log(f"[{label}] Chunks: {stats[speaker].get('chunks', 0)}, "
                    f"Transcriptions: {stats[speaker].get('transcriptions', 0)}, "
                    f"Keywords: {stats[speaker].get('keywords', 0)}")
        
    except Exception as e:
        log(f"Error saving results: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    # ëª¨ë¸ ë¡œë“œ
    load_models()
    
    # ì„œë²„ ì‹œì‘
    log("Starting server...")
    log(f"Server will listen on port {HTTP_SERVER_PORT}")
    app.run(host='0.0.0.0', port=HTTP_SERVER_PORT, debug=True, use_reloader=False)
