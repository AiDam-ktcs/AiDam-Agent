#!/usr/bin/env python3
"""
HuggingFaceì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ì— ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python download_keyword_model.py

ì„¤ëª…:
    - Qwen/Qwen2.5-1.5B ëª¨ë¸ì„ HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ
    - backend/STT/models/qwen3-1.7b í´ë”ì— ì €ì¥
    - ì•½ 3GB ë‹¤ìš´ë¡œë“œ (ì¸í„°ë„· ì†ë„ì— ë”°ë¼ 5-10ë¶„ ì†Œìš”)
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
SCRIPT_DIR = Path(__file__).parent
BASE_DIR = SCRIPT_DIR.parent
MODELS_DIR = BASE_DIR / 'models'
TARGET_MODEL_DIR = MODELS_DIR / 'qwen3-1.7b'

# HuggingFace ëª¨ë¸ ID
HUGGINGFACE_MODEL_ID = "Qwen/Qwen3-1.7B"

def print_header():
    """í—¤ë” ì¶œë ¥"""
    print("=" * 70)
    print("ğŸ¤– Keyword Extraction Model Downloader")
    print("=" * 70)
    print(f"Model: {HUGGINGFACE_MODEL_ID}")
    print(f"Target Directory: {TARGET_MODEL_DIR}")
    print(f"Expected Size: ~3GB")
    print("=" * 70)
    print()

def check_existing_model():
    """ì´ë¯¸ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸"""
    model_files = ['pytorch_model.bin', 'model.safetensors']
    
    if TARGET_MODEL_DIR.exists():
        for model_file in model_files:
            if (TARGET_MODEL_DIR / model_file).exists():
                print(f"âœ“ Model already exists: {TARGET_MODEL_DIR / model_file}")
                return True
    
    return False

def download_model():
    """HuggingFaceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        print(f"ğŸ“¥ Downloading model from HuggingFace: {HUGGINGFACE_MODEL_ID}")
        print("   This may take 5-10 minutes depending on your internet speed...")
        print()
        
        # models ë””ë ‰í† ë¦¬ ìƒì„±
        MODELS_DIR.mkdir(parents=True, exist_ok=True)
        
        # í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
        print("1/2 Downloading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL_ID)
        tokenizer.save_pretrained(TARGET_MODEL_DIR)
        print("âœ“ Tokenizer saved successfully")
        print()
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
        print("2/2 Downloading model (this will take a few minutes)...")
        model = AutoModelForCausalLM.from_pretrained(
            HUGGINGFACE_MODEL_ID,
            torch_dtype="auto",
            device_map="auto"
        )
        model.save_pretrained(TARGET_MODEL_DIR)
        print("âœ“ Model saved successfully")
        print()
        
        return True
        
    except ImportError as e:
        print("âŒ Error: Required packages not installed")
        print("   Please install transformers and torch:")
        print("   pip install transformers torch")
        print()
        print(f"   Details: {e}")
        return False
        
    except Exception as e:
        print(f"âŒ Error downloading model: {e}")
        import traceback
        traceback.print_exc()
        return False

def verify_download():
    """ë‹¤ìš´ë¡œë“œ í™•ì¸"""
    print("\n" + "=" * 70)
    print("ğŸ“‚ Verifying downloaded files...")
    print("=" * 70)
    
    required_files = [
        'config.json',
        'tokenizer.json',
        'vocab.json',
        'merges.txt',
    ]
    
    model_weight_files = ['pytorch_model.bin', 'model.safetensors']
    
    all_good = True
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸
    for file_name in required_files:
        file_path = TARGET_MODEL_DIR / file_name
        if file_path.exists():
            size = file_path.stat().st_size / (1024 * 1024)  # MB
            print(f"âœ“ {file_name:<30} ({size:.2f} MB)")
        else:
            print(f"âœ— {file_name:<30} (MISSING)")
            all_good = False
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ í™•ì¸ (ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ìˆì–´ë„ ë¨)
    has_model_weights = False
    for file_name in model_weight_files:
        file_path = TARGET_MODEL_DIR / file_name
        if file_path.exists():
            size = file_path.stat().st_size / (1024 * 1024 * 1024)  # GB
            print(f"âœ“ {file_name:<30} ({size:.2f} GB)")
            has_model_weights = True
    
    if not has_model_weights:
        print(f"âœ— Model weights (pytorch_model.bin or model.safetensors) (MISSING)")
        all_good = False
    
    print("=" * 70)
    
    if all_good:
        print("\nâœ… All files downloaded successfully!")
        print(f"\nğŸ“ Model saved to: {TARGET_MODEL_DIR}")
        print("\nğŸš€ You can now run the STT server:")
        print("   cd backend/STT/call_stt")
        print("   python server5.py")
    else:
        print("\nâš ï¸  Some files are missing. Please try downloading again.")
    
    print()
    return all_good

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print_header()
    
    # ì´ë¯¸ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
    if check_existing_model():
        print("\nâš ï¸  Model already exists!")
        response = input("Do you want to re-download? (y/N): ").strip().lower()
        if response != 'y':
            print("Skipping download.")
            return 0
        print()
    
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    success = download_model()
    
    if not success:
        return 1
    
    # ë‹¤ìš´ë¡œë“œ í™•ì¸
    verify_download()
    
    return 0

if __name__ == '__main__':
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Download interrupted by user.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

